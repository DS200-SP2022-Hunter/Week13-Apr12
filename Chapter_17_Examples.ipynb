{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter_17_Examples.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Chapter 17 classification examples\n",
        "\n",
        "This notebook uses some existing tools, other than those used by the textbook, to implement the examples seen in Chapter 17."
      ],
      "metadata": {
        "id": "0Rv5y0DGL8cL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {
        "id": "FuS4uaU9L4Li"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "path_data = 'http://personal.psu.edu/drh20/200DS/assets/data/'"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 17.1. Nearest Neighbors\n",
        "\n",
        "[Section 17.1](https://inferentialthinking.com/chapters/17/1/Nearest_Neighbors.html) uses the `ckd` dataset on patients with or without chronic kidney disease to build a nearest-neighbor classifier.  We'll use the `sklearn` implementation of nearest neighbors."
      ],
      "metadata": {
        "id": "tr8F2odnM_7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import neighbors"
      ],
      "metadata": {
        "id": "shL6yrkeLIGc"
      },
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read in the CKD dataset, then create the predictor dataframe and response array:"
      ],
      "metadata": {
        "id": "Zy2LrRYsLTve"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ckd = pd.read_csv(path_data + 'ckd.csv')\n",
        "ckd.columns"
      ],
      "metadata": {
        "id": "G0n8o-UYJLRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ckd_pred = ckd.iloc[:, [14, 9]] # Grab the Hemoglobin and Glucose columns\n",
        "ckd_pred = ckd_pred.rename(columns = {'Blood Glucose Random': 'Glucose'})\n",
        "ckd_pred = (ckd_pred-ckd_pred.mean())/ckd_pred.std() # Standardize each column\n",
        "ckd_resp = np.array(ckd.iloc[:, 24]) # Grab the Class column\n",
        "ckd_pred # Take a look at the predictor DataFrame"
      ],
      "metadata": {
        "id": "hbP_zVdgM8iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now a scatterplot of Hemoglobin vs. Glucose:"
      ],
      "metadata": {
        "id": "xK88JME9yGiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "for resp, color, lab in zip([0, 1], ['blue', 'orange'], ['No CKD', 'CKD']):\n",
        "  x = ckd_pred.loc[ckd_resp==resp, 'Hemoglobin']\n",
        "  y = ckd_pred.loc[ckd_resp==resp, 'Glucose']\n",
        "  plt.scatter(x=x, y=y, c=color, label=lab)\n",
        "  \n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6q_ZuqJywhrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, define a new patient named Alice and give her (standardized versions of) hemoglobin and glucose:"
      ],
      "metadata": {
        "id": "93W37wtlSTVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alice = pd.DataFrame(data={'Hemoglobin': [0.0], 'Glucose': [1.5]})\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "for resp, color, lab in zip([0, 1], ['blue', 'orange'], ['No CKD', 'CKD']):\n",
        "  x = ckd_pred.loc[ckd_resp==resp, 'Hemoglobin']\n",
        "  y = ckd_pred.loc[ckd_resp==resp, 'Glucose']\n",
        "  plt.scatter(x=x, y=y, c=color, label=lab)  \n",
        "\n",
        "plt.scatter(x=alice.loc[0,'Hemoglobin'], y=alice.loc[0,'Glucose'], c='red', label='Alice')\n",
        "\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XOK--_ZCSdFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can train a k-nearest neighbors classifier using k=1, then use it to classify Alice.  From the look of the plot, Alice's nearest neighbor is a CKD individual, which means that she should be classifed as 1:"
      ],
      "metadata": {
        "id": "e9eyVx11TAqw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knn1 = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
        "knn1.fit(ckd_pred, ckd_resp)\n",
        "knn1.predict(alice)"
      ],
      "metadata": {
        "id": "Zi0NSMbIy0z_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can check that for a 1-nearest neighbor classifier, we get a 100% \"success\" rate if we apply it to the training dataset:"
      ],
      "metadata": {
        "id": "khxGHcHObfTg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knn1.predict(ckd_pred) == ckd_resp"
      ],
      "metadata": {
        "id": "38Km8AxMbeNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If instead we train a 5-nearest neighbors classifier, we no longer get 100% correct in the training set:"
      ],
      "metadata": {
        "id": "v_ZuElepbtvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knn5 = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
        "knn5.fit(ckd_pred, ckd_resp)\n",
        "knn5.predict(ckd_pred) == ckd_resp"
      ],
      "metadata": {
        "id": "lL8Bm4uYZD_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can create a mesh of grid points, then classify each one, and this will give us a decision boundary:"
      ],
      "metadata": {
        "id": "D9TkvSJacObI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h = 0.02  # step size in the mesh\n",
        "\n",
        "x_min, x_max = ckd_pred.iloc[:, 0].min() - 1, ckd_pred.iloc[:, 0].max() + 1\n",
        "y_min, y_max = ckd_pred.iloc[:, 1].min() - 1, ckd_pred.iloc[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "mesh = pd.DataFrame().assign(Hemoglobin=xx.ravel(), Glucose=yy.ravel())\n",
        "Z = knn1.predict(mesh)\n",
        "\n",
        "# Put the result into a color plot\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "\n",
        "for resp, color, lab in zip([0, 1], ['blue', 'orange'], ['No CKD', 'CKD']):\n",
        "  x = ckd_pred.loc[ckd_resp==resp, 'Hemoglobin']\n",
        "  y = ckd_pred.loc[ckd_resp==resp, 'Glucose']\n",
        "  plt.scatter(x=x, y=y, c=color, label=lab, edgecolor='black')\n",
        "  \n",
        "plt.legend()\n",
        "plt.title('1-Nearest Neighbor Classifier')\n",
        "plt.xlabel('Hemoglobin standard units')\n",
        "plt.ylabel('Glucose standard units')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IVkHiIpazw0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 17.2. Training and Testing\n",
        "\n",
        "[Section 17.2](https://inferentialthinking.com/chapters/17/2/Training_and_Testing.html) also uses the CKD dataset, but with different columns chosen as predictors as compared with [Section 17.1](https://inferentialthinking.com/chapters/17/1/Nearest_Neighbors.html). "
      ],
      "metadata": {
        "id": "P1jlBAoVH1Gj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ckd_pred = ckd.iloc[:, [16, 9]] # Grab the White Blood Cell Count and Glucose columns\n",
        "ckd_pred = ckd_pred.rename(columns = {'Blood Glucose Random': 'Glucose'})\n",
        "ckd_pred = (ckd_pred-ckd_pred.mean())/ckd_pred.std() # Standardize each column\n",
        "ckd_resp = np.array(ckd.iloc[:, 24]) # Grab the Class column"
      ],
      "metadata": {
        "id": "d6Z5a9IaI84u"
      },
      "execution_count": 261,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a scatterplot of the two variables:"
      ],
      "metadata": {
        "id": "8eKKK_1tdz6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "for resp, color, lab in zip([0, 1], ['blue', 'orange'], ['No CKD', 'CKD']):\n",
        "  x = ckd_pred.loc[ckd_resp==resp, 'White Blood Cell Count']\n",
        "  y = ckd_pred.loc[ckd_resp==resp, 'Glucose']\n",
        "  plt.scatter(x=x, y=y, c=color, label=lab)\n",
        "  \n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.xlabel('White Blood Cell Count')\n",
        "plt.ylabel('Glucose')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Uyrcy5ApeBBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can randomly split the dataset into a training set and an equal-sized test set:"
      ],
      "metadata": {
        "id": "88j28CnVed01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shuffle = np.random.choice(158, 158, replace=False)\n",
        "train = shuffle[0:79] # this is the first half of a randomly reshuffled array from 0 to 157\n",
        "test = shuffle[79:158] # this is the second half\n",
        "train_pred = ckd_pred.iloc[train, :]\n",
        "train_resp = ckd_resp[train]\n",
        "test_pred = ckd_pred.iloc[test,:]\n",
        "test_resp = ckd_resp[test]"
      ],
      "metadata": {
        "id": "43pqwYg_gLP0"
      },
      "execution_count": 263,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now train a 1-nearest neighbor classifier using the training set and check how it does on the test set:"
      ],
      "metadata": {
        "id": "D2LcjkPll7PS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knn1 = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
        "knn1.fit(train_pred, train_resp)\n",
        "knn1.predict(test_pred) == test_resp"
      ],
      "metadata": {
        "id": "KLubewUymCf3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can depict the decision boundary and the test set on the same plot:"
      ],
      "metadata": {
        "id": "M37JSPeDmvOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "h = 0.02  # step size in the mesh\n",
        "\n",
        "x_min, x_max = train_pred.iloc[:, 0].min() - 1, train_pred.iloc[:, 0].max() + 1\n",
        "y_min, y_max = train_pred.iloc[:, 1].min() - 1, train_pred.iloc[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "mesh = pd.DataFrame().assign(a=xx.ravel(), Glucose=yy.ravel()).rename(columns = {'a': 'White Blood Cell Count'})\n",
        "Z = knn1.predict(mesh) # This makes sure that the background contour plot is based on training data\n",
        "\n",
        "# Put the result into a color plot\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "\n",
        "# Now add test points\n",
        "for resp, color, lab in zip([0, 1], ['blue', 'orange'], ['No CKD', 'CKD']):\n",
        "  x = test_pred.loc[test_resp==resp, 'White Blood Cell Count']\n",
        "  y = test_pred.loc[test_resp==resp, 'Glucose']\n",
        "  plt.scatter(x=x, y=y, c=color, label=lab, edgecolor='black')\n",
        "  \n",
        "plt.legend()\n",
        "plt.title('1-Nearest Neighbor Classifier')\n",
        "plt.xlabel('White Blood Cell Count standard units')\n",
        "plt.ylabel('Glucose standard units')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dEMJDy6XkJEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 17.3. Rows of Tables\n",
        "\n",
        "Much of [Section 17.3](https://inferentialthinking.com/chapters/17/3/Rows_of_Tables.html) uses the capabilities of `numpy`, rather than the `datascience` library.  Thus, we can mostly repeat what was done in that section without much change.  "
      ],
      "metadata": {
        "id": "VcC0j8kEpdHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ckd_pred = ckd.iloc[:, [14, 9]] # Grab the Hemoglobin and Glucose columns\n",
        "ckd_pred = ckd_pred.rename(columns = {'Blood Glucose Random': 'Glucose'})\n",
        "ckd_pred = (ckd_pred-ckd_pred.mean())/ckd_pred.std() # Standardize each column\n",
        "ckd_resp = np.array(ckd.iloc[:, 24]) # Grab the Class column"
      ],
      "metadata": {
        "id": "VA5HRhHBtJhg"
      },
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a new point called `Alice` and a distance function used to calculate the distance between Alice and a particular row of the `ckd_pred` dataset:"
      ],
      "metadata": {
        "id": "jRblF5-VtKHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alice = np.array([0, 1.1])\n",
        "def distance(point1, point2): #Returns the Euclidean distance between point1 and point2.\n",
        "    return np.sqrt(np.sum((point1 - point2)**2))\n",
        "distance(alice, ckd_pred.iloc[46,:])"
      ],
      "metadata": {
        "id": "9SMKVs-JrqZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can create a new function called `distance_from_alice` just like in [Section 17.3](https://inferentialthinking.com/chapters/17/3/Rows_of_Tables.html), then apply this function to each row in `ckd_pred`.  The main difference between this code and the code in the textbook is that the `apply` function here works on a pandas `DataFrame` object, and this means that we have to tell it which set of indices (in this case, the column indices or `axis=1`) to use:"
      ],
      "metadata": {
        "id": "pnbS_uAZ6xoa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def distance_from_alice(row): #Returns distance between Alice and a row of predictors\n",
        "    return distance(alice, np.array(row))\n",
        "distances = ckd_pred.apply(distance_from_alice, axis=1)\n",
        "\n",
        "# Now display a DataFrame in which the distance column is added, then used to sort:\n",
        "ckd_pred.assign(distance_from_alice=distances).sort_values('distance_from_alice')"
      ],
      "metadata": {
        "id": "0962fGIj68GL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can produce the final plot from [Section 17.3](https://inferentialthinking.com/chapters/17/3/Rows_of_Tables.html) without changing much of the original code from the textbook.  This plot shows Alice surrounded by a circle of radius equal to the fifth-smallest distance:"
      ],
      "metadata": {
        "id": "38NhZll4B64T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,8))\n",
        "plt.scatter(ckd_pred['Hemoglobin'], ckd_pred['Glucose'], c=ckd_resp, s=40)\n",
        "plt.scatter(alice[0], alice[1], c='red', s=40)\n",
        "radius = np.sort(distances)[4]\n",
        "theta = np.arange(0, 2*np.pi+1, 2*np.pi/200)\n",
        "plt.plot(radius*np.cos(theta)+alice[0], radius*np.sin(theta)+alice[1], c='g', lw=1.5);\n",
        "plt.xlim(-2, 2.5)\n",
        "plt.ylim(-2, 2.5)\n",
        "plt.grid(True);"
      ],
      "metadata": {
        "id": "rMQdokMdAmej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 17.4. Implementing the Classifier\n",
        "\n",
        "[Section 17.4](https://inferentialthinking.com/chapters/17/4/Implementing_the_Classifier.html) begins with a new dataset with measurements on banknotes, some of which are counterfeit and some of which are not.  Here is code that reads in the dataset and then produces a scatterplot like the first one in Section 17.4:"
      ],
      "metadata": {
        "id": "UfuttoE3EEtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "banknotes = pd.read_csv(path_data + 'banknote.csv')\n",
        "notes_resp = np.array(banknotes['Class'])\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "for resp, color, lab in zip([0, 1], ['blue', 'orange'], ['Genuine', 'Counterfeit']):\n",
        "  x = banknotes.loc[notes_resp==resp, 'WaveletVar']\n",
        "  y = banknotes.loc[notes_resp==resp, 'WaveletCurt']\n",
        "  plt.scatter(x=x, y=y, c=color, label=lab)\n",
        "  \n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.xlabel('WaveletVar')\n",
        "plt.ylabel('WaveletCurt')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "td8SLJqoELJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And here is a 3-d plot like the one seen in [Section 17.4](https://inferentialthinking.com/chapters/17/4/Implementing_the_Classifier.html):"
      ],
      "metadata": {
        "id": "uvmfM8sXG_cx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(8, 6))\n",
        "ax = Axes3D(fig)\n",
        "\n",
        "for grp_name, grp_idx in banknotes.groupby('Class').groups.items():\n",
        "    x = banknotes.loc[grp_idx,'WaveletVar']\n",
        "    y = banknotes.loc[grp_idx,'WaveletSkew']\n",
        "    z = banknotes.loc[grp_idx,'WaveletCurt']\n",
        "    ax.scatter(x,y,z, label=grp_name)\n",
        "\n",
        "ax.legend(labels=['Genuine', 'Counterfeit']);"
      ],
      "metadata": {
        "id": "Sfdg7dgJG-y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, here is an interactive version of the same 3-d plot:"
      ],
      "metadata": {
        "id": "FFbTOAlGHgH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "df = banknotes.assign(size=2.0)\n",
        "fig = px.scatter_3d(df, x='WaveletVar', y='WaveletSkew', z='WaveletCurt',\n",
        "              color='Class', size = 'size', opacity=0.5)\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "L_FPcfwDHpE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, [Section 17.4](https://inferentialthinking.com/chapters/17/4/Implementing_the_Classifier.html) considers the `wine` dataset.  This code reads the dataset and splits it into predictors and response:"
      ],
      "metadata": {
        "id": "arlqkpl0LDkt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wines = pd.read_csv(path_data + 'wine.csv')\n",
        "wine_pred = wines.iloc[:,1:14] # columns 2 through 14 are predictors\n",
        "wine_resp = wines.iloc[:,0] # column 1 is the response (category of wine)"
      ],
      "metadata": {
        "id": "DX17ITYuLat5"
      },
      "execution_count": 272,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is an illustration that we can use the same `distance` function defined in [Section 17.3](https://inferentialthinking.com/chapters/17/3/Rows_of_Tables.html) on 14-dimensional predictors like in this example:"
      ],
      "metadata": {
        "id": "esdRM7gQL6F8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "distance(wine_pred.iloc[0,:], wine_pred.iloc[1,:])"
      ],
      "metadata": {
        "id": "aGasUUX_MQ4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Section 17.4.5, several functions are defined.  We can create versions of each of them.  The `distance` function has already been defined above."
      ],
      "metadata": {
        "id": "_1j2LkBTNTfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def all_distances(training, new_point): # Return array of dist between each training point and new_point\n",
        "    def distance_from_new_point(row):\n",
        "        return distance(np.array(new_point), np.array(row))\n",
        "    return training.apply(distance_from_new_point, axis=1)\n",
        "\n",
        "def table_with_distances(training, new_point): # Return DataFrame with all_distances appended\n",
        "    return training.assign(Distance=all_distances(training, new_point))\n",
        "\n",
        "def closest(training, new_point, k): # Return array of indices of k closest rows \n",
        "    return np.array(table_with_distances(training, new_point).sort_values('Distance').index[0:k])"
      ],
      "metadata": {
        "id": "-G4LBCPPNdaI"
      },
      "execution_count": 274,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To duplicate what happens in [Section 17.4](https://inferentialthinking.com/chapters/17/4/Implementing_the_Classifier.html), let's take row 0 to be the `new_point` and find the indices of the 5 closest rows to that point.  (Which row index will show up as the closest row?)"
      ],
      "metadata": {
        "id": "cw-q0T9NPbGE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "five_closest = closest(wine_pred, wine_pred.iloc[0,:], 5)\n",
        "five_closest"
      ],
      "metadata": {
        "id": "GMIPNEvcO8AA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "By referring to the original dataset and looking at the 5 closest rows, we can see that the response values of those 5 nearest rows are all the same, which means that a 5-nearest neighbors classifier would consider row 0 to have `Class` equal to 1:"
      ],
      "metadata": {
        "id": "RWhQrttiP-x0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wines.iloc[five_closest,:]"
      ],
      "metadata": {
        "id": "a5UdVJTZQDgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember, we don't have to implement the k-nearest neighbors classifier ourselves like the textbook does.  We can use the `sklearn` library's code for this purpose instead."
      ],
      "metadata": {
        "id": "5AO2k5O1Qphc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 17.5. The accuracy of the classifier\n",
        "\n",
        "[Section 17.5](https://inferentialthinking.com/chapters/17/5/Accuracy_of_the_Classifier.html) begins by splitting the `wines` dataset into two equal-sized datasets, one for training and one for testing:"
      ],
      "metadata": {
        "id": "vIRF9kRRRIMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shuffle = np.random.choice(178, 178, replace=False)\n",
        "train = shuffle[0:89] # this is the first half of a randomly reshuffled array from 0 to 177\n",
        "test = shuffle[89:178] # this is the second half\n",
        "train_pred = wine_pred.iloc[train, :]\n",
        "train_resp = wine_resp[train]\n",
        "test_pred = wine_pred.iloc[test,:]\n",
        "test_resp = wine_resp[test]"
      ],
      "metadata": {
        "id": "7QEH4pj9RSaE"
      },
      "execution_count": 287,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum(test_resp) + np.sum(train_resp) - np.sum(wine_resp)"
      ],
      "metadata": {
        "id": "YHG_gTRcS7X4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can train the 5-nearest neighbor classifier and test it:"
      ],
      "metadata": {
        "id": "2Gde2Cc9yJqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knn5 = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
        "knn5.fit(train_pred, train_resp)\n",
        "np.array(knn5.predict(test_pred) == test_resp)"
      ],
      "metadata": {
        "id": "6vsHEPVayTjN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Taking the mean of True/False is a fast way to see the proportion of True:"
      ],
      "metadata": {
        "id": "vCHYDkIvyody"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(knn5.predict(test_pred) == test_resp)"
      ],
      "metadata": {
        "id": "uJ0XmoArytxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our accuracy is slightly lower than the accuracy seen in the textbook because the textbook reduced the number of wine categories from 3 to 2."
      ],
      "metadata": {
        "id": "z-AyEGD3y-Qh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 17.6. Multiple Regression\n",
        "[Section 17.6](https://inferentialthinking.com/chapters/17/6/Multiple_Regression.html) uses the `minimize` function to minimize the RMSE (root mean squared error) as a function of the 9-dimensional array of slope values.  However, the minimizer can be found directly using multivariable calculus, which is the way that all regression software actually works.  You don't need to understand the calculus for this, but a bit of matrix notation knowledge helps:\n",
        "\n",
        "The slopes that minimize the mean square error are given by\n",
        "$$ (X^\\top X)^{-1} X^\\top Y, $$\n",
        "where $X$ is the matrix of predictors, $Y$ is the vector (array) of response values, $X^\\top$ is the transpose of $X$, and $(X^\\top X)^{-1}$ is the matrix inverse of $X^\\top X$.\n",
        "\n",
        "Let's read in the house price dataset and set up the predictor matrix and response array:"
      ],
      "metadata": {
        "id": "JP9HjR_UTSqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "house = pd.read_csv(path_data + 'house.csv')\n",
        "\n",
        "# First, filter so we only get the rows (houses) we want:\n",
        "house = house.loc[house['Bldg Type']=='1Fam', :] # only 1Fam houses\n",
        "house = house.loc[house['Sale Condition']=='Normal', :] # only Normal sale condition\n",
        "\n",
        "# Second, select only the subset of columns (variables) we want:\n",
        "house = house[['SalePrice', '1st Flr SF', '2nd Flr SF', \n",
        "    'Total Bsmt SF', 'Garage Area', \n",
        "    'Wood Deck SF', 'Open Porch SF', 'Lot Area', \n",
        "    'Year Built', 'Yr Sold']]\n",
        "\n",
        "# Third, sort by sale price (to match Section 17.6):\n",
        "house = house.sort_values('SalePrice')\n",
        "\n",
        "# Finally, create house_pred and house_resp arrays:\n",
        "house_pred = np.array(house.iloc[:, 1:10]) # Convert the DataFrame to an array here!\n",
        "house_resp = np.array(house.iloc[:, 0])"
      ],
      "metadata": {
        "id": "GKMh-E2yYG86"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the datasets into training and test of equal size:"
      ],
      "metadata": {
        "id": "q1CrfTuSl05B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shuffle = np.random.choice(2002, 2002, replace=False)\n",
        "train = shuffle[0:1001] # this is the first half of a randomly reshuffled array from 0 to 177\n",
        "test = shuffle[1001:2002] # this is the second half\n",
        "train_pred = house_pred[train, :] # don't need '.iloc' here because it's an array, not a DataFrame\n",
        "train_resp = house_resp[train]\n",
        "test_pred = house_pred[test,:] # don't need '.iloc' here because it's an array, not a DataFrame\n",
        "test_resp = house_resp[test]"
      ],
      "metadata": {
        "id": "qkAhplrjl8Lb"
      },
      "execution_count": 229,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can use the formula above to find the best slopes:  $(X^\\top X)^{-1} X^\\top Y$.  Compare with the values in Section 17.6.2.1, keeping in mind that they will be slightly different because the training subset is randomly selected:"
      ],
      "metadata": {
        "id": "G83YpuD8mVRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = train_pred\n",
        "Y = train_resp\n",
        "XtXinv = np.linalg.inv(np.matmul(np.transpose(X), X))\n",
        "XtY = np.matmul(np.transpose(X), Y)\n",
        "best_slopes = np.matmul(XtXinv, XtY)\n",
        "np.round(best_slopes)"
      ],
      "metadata": {
        "id": "dpBqMwBos46W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To predict a sales price based on an array of predictor values, we can use the same `predict` function from Section 17.6.2:"
      ],
      "metadata": {
        "id": "i2BsQzO2oY2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(slopes, row):\n",
        "    return sum(slopes * np.array(row))\n",
        "\n",
        "row = 0\n",
        "test_row = test_pred[row,:]\n",
        "print('Actual price ', test_resp[row])\n",
        "print('Predicted price ', predict(best_slopes, test_pred[row, :]) )"
      ],
      "metadata": {
        "id": "xfgfnEDXowTq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}